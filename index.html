<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <link rel="author" href="https://like413.github.io/CDQAG/">
  <title>Show Me What and Where has Changed? Question Answering and Grounding for Remote Sensing Change Detection</title>
  <meta name="description" content="Show Me What and Where has Changed? Question Answering and Grounding for Remote Sensing Change Detection">
  <meta name="keywords" content="GRES; Referring Image Segmentation; Generalized Referring Expression Segmentation; Generalized Referring Expression Comprehension; GRES Dataset; CVPR 2023; Henghui Ding; Nanyang Technological University; Computer Vision">
  
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Show Me What and Where has Changed? Question Answering and Grounding for Remote Sensing Change Detection">
  <meta property="og:title" content="Show Me What and Where has Changed? Question Answering and Grounding for Remote Sensing Change Detection"/>
  <meta property="og:description" content="Show Me What and Where has Changed? Question Answering and Grounding for Remote Sensing Change Detection"/>
  <meta property="og:url" content="https://like413.github.io/CDQAG/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1800"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Show Me What and Where has Changed? Question Answering and Grounding for Remote Sensing Change Detection">
  <meta name="twitter:description" content="Show Me What and Where has Changed? Question Answering and Grounding for Remote Sensing Change Detection">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="GRES; Referring Image Segmentation; Generalized Referring Expression Segmentation; Generalized Referring Expression Comprehension; GRES Dataset; CVPR 2023; ; Henghui Ding; Nanyang Technological University; Computer Vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  
  <link rel="icon" type="image/x-icon" href="static/favicon_io/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="static/favicon_io/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/favicon_io/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/favicon_io/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://henghuiding.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Challenges
        </a>
        <div class="navbar-dropdown">
           <a class="navbar-item" href="https://henghuiding.github.io/MOSE/ChallengeCVPR2024">
            1st MOSE Challenge on CVPR 2024
          </a>
          <a class="navbar-item" href="https://henghuiding.github.io/MeViS/ChallengeCVPR2024">
            1st MeViS Challenge on CVPR 2024
          </a>
          <a class="navbar-item" href="https://henghuiding.github.io/MOSE">
            MOSE Dataset Page
          </a>
          <a class="navbar-item" href="https://henghuiding.github.io/MeViS">
            MeViS Dataset Page
          </a>
          <a class="navbar-item" href="https://henghuiding.github.io/GRES">
            GRES Dataset Page
          </a>
        </div>
      </div>
    </div>
  </div> -->
</nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Show Me What and Where has Changed? <br>
              Question Answering and Grounding for Remote Sensing Change Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://like413.github.io/CDQAG">Chang Liu</a>,&nbsp;</span>
               <span class="author-block">
                <a href="https://henghuiding.github.io/" target="_blank">Henghui Ding</a><sup><object id="object" data="static/images/envelope.svg" width="16" height="16" type="image/svg+xml"></object> </sup>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://personal.ntu.edu.sg/exdjiang/" target="_blank">Xudong Jiang</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      Xidian University, Chongqing University of Posts and Telecommunications
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      National University of Singapore
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                  <object id="object" data="static/images/envelope.svg" width="20" height="20" type="image/svg+xml" style="position:relative;top:3px;"></object> Project Leader & Corresponding Author
                  </div>                  

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
<!--                       <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span> -->
                      <!-- </a> -->
                    <!-- </span> -->
                     <span class="link-block">
                      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>PDF</span>
                    </a>
                  </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://github.com/henghuiding/gRefCOCO" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
<!--                       <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span> -->
                      <span>üî•Dataset</span>
                    </a>
                  </span>
<!--                   <span class="link-block">
                      <a href="https://codalab.lisn.upsaclay.fr/competitions/10703" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span>üî•Eval Server</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/henghuiding/ReLA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2306.00968" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>GRES arXiv</span>
                </a>
              </span>

              <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2308.16182" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>GREC arXiv</span>
                </a>
              </span> -->

              <!-- Youtube link -->
                  <!-- <span class="link-block">
                    <a href="https://www.youtube.com/watch?v=COJVqz_sgNU" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>YouTube</span>
                  </a>
                </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <center><img src="static/DemoImages/1.png" border="0" width="100%"></center>
      <!-- <h2 class="subtitle has-text-centered"> -->
        <div align="center"><p style="text-align:justify; text-justify:inter-ideograph;width:100%">Figure 1. Change detection (CD) identifies surface changes from multi-temporal images. Classic visual question answering (VQA) only supports textual answers. Compared with them, the proposed change detection qusetion answering and grounding (<b>CDQAG</b>) supports well-founded answers, i.e., textual answers (‚Äúwhat has changed‚Äù) and relevant visual feedback (‚Äúwhere has changed‚Äù)</p></div><br> 

               <!-- <h2 class="title">News</h2>
                <HR color=#F0F0F0 width="97%" SIZE=1>
                <div class="news" style="overflow:auto; height:200px; Width:99%;padding-top: 6px;">
                <ul>
                  <li>[03, 2024]&nbsp;&nbsp;&nbsp; <b>Call for Papers</b>: <a href="https://www.vspwdataset.com/Workshop2024" target="_blank"><font color="#FF6403"><b>CVPR 2024 PVUW Workshop</b></font></a>.
                <li>[02, 2024]&nbsp;&nbsp;&nbsp; <a href="https://henghuiding.github.io/MeViS/ChallengeCVPR2024" target="_blank"><b>MeViS challenge</b></a></font> and <a href="https://henghuiding.github.io/MOSE/ChallengeCVPR2024" target="_blank"><b>MOSE challenge</b></a> will be held in conjunction with <a href="https://www.vspwdataset.com/Workshop2024" target="_blank"><font color="#FF6403"><b>CVPR 2024 PVUW Workshop</b></font></a>.
                <li>[08, 2023]&nbsp;&nbsp;&nbsp; <font color="#FF6403"><b>Generalized Referring Comprehension</b> (<b>GREC</b>)</font> is ready, please see the <a href="https://arxiv.org/abs/2308.16182" target="_blank">technical report</a> and <a href="https://github.com/henghuiding/gRefCOCO" target="_blank">code</a>.</li>
                <li>[08, 2023]&nbsp;&nbsp;&nbsp; <a href="https://henghuiding.github.io/MeViS" target="_blank"><b>MeViS Dataset</b></a> is released, which supports expressions referring to multiple
object(s) in the video.</li>
                <li>[08, 2023]&nbsp;&nbsp;&nbsp; We have updated and reorganized the dataset file. Please download the latest version for train/val/testA/testB!</li>
                <li>[06, 2023]&nbsp;&nbsp;&nbsp; <a href="https://github.com/henghuiding/gRefCOCO" target="_blank">gRefCOCO Dataset</a> and GRES method <a href="https://github.com/henghuiding/ReLA" target="_blank">code</a> are released.</li>
                <li>[03, 2023]&nbsp;&nbsp;&nbsp; GRES was selected as a <b>highlight at CVPR 2023</b>, acceptance rate 2.57%.</li>
                <li>[02, 2023]&nbsp;&nbsp;&nbsp; GRES accepted to CVPR 2023.</li>


                </ul>
             </div>   -->

</div>
  </div>
</section>

<!-- End teaser video -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="text-align:justify; text-justify:inter-ideograph;">
            Remote sensing change detection aims to perceive changes occurring on the Earth's surface from remote sensing data in different periods, and feeds these changes back to humans. However, most existing methods only focus on detecting change regions, lacking the ability to interact with users for identifying changes that user expected. In this paper, we introduce a new benchmark named <b>C</b>hange <b>D</b>etection <b>Q</b>uestion <b>A</b>nswering and <b>G</b>rounding (<b>CDQAG</b>), which extends the traditional change detection task by providing interpretable textual answers and intuitive visual evidence. To this end, we construct the first CDQAG dataset, termed <b>AGC-360K</b>, comprising over 360K triplets of questions, textual answers, and corresponding high-quality visual masks. Based on this, we propose a simple yet effective baseline model named <b>QAGFormer</b> that aims to accomplish the unified tasks of question answering and question grounding. Extensive qualitative and quantitative experimental results provide useful insights for the development of better CDQAG models, and we hope that our work can inspire further research in this important yet underexplored direction. 
    </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



    </div>
<section class="section" id="Visualization">
    <div class="container is-max-desktop content">
      <h2 class="title">GRES Setting</h2>
      Generalized Referring Expression Segmentation (GRES) allows expressions indicating <b>any number of target objects</b>. GRES takes an image and a referring expression as input, and requires mask prediction of the target object(s). 
      <!-- Different from classic RES, as shown in Figure 1, GRES further supports multi-target expression that specifies multiple target objects in a single expression, <i>eg</i>, <i>``Everyone except the kid in white''</i>, and no-target expression that does not touch on any object in the image, <i>eg</i>, <i>``the kid in blue''</i>. This provides much more flexibility for input expression, making referring expression segmentation more useful and robust in practice.-->
      <br><br> 
      <b>&#9734; Multi-object expressions</b>: an expression indicates multiple target objects.

      <br>

      <b>&#9734; No-target expressions</b>: an expression does not touch on any object in the image.

      <br>

      <b>&#9734; Single-target expressions</b>: an expression indicates a single target object.
      <br><br>
     <center><img src="static/DemoImages/Figure2a.png" border="0" width="64%"></center><br>
     <center><img src="static/DemoImages/Figure2b.png" border="0" width="64%"></center><br>
        <div align="center"><p style="text-align:justify; text-justify:inter-ideograph;width:100%">Figure 2. More applications of GRES brought by supporting multi-target and no-target expressions compared to classic RES.</p></div><br> 
    </div>
</section>



<section class="section" id="Experiments">
  <div class="container is-max-desktop content">
  <h2 class="title">Experiments</h2>
    <!-- <center> -->
      We benchmark the state-of-the-art methods on AGC-360K and CDVQA datasets to the best of our knowledge.<br><br>

     <center><caption><b>TABLE 1. QAGFormer results: comparison on AGC-360K dataset</b></caption></center>
     <center><img src="static/DemoImages/table1.png" border="0" width="80%"></center><br><br>

     <center><caption><b>TABLE 2. QAGFormer results: comparison on CDVQA dataset</b></caption></center>
     <center><img src="static/DemoImages/table2.png" border="0" width="99%"></center>

    <!-- </center> -->
    </div>
</section>

<!-- End image carousel -->
<section class="section" id="Downloads">
  <div class="container is-max-desktop content">
  <h2 class="title">Downloads</h2>
    <center>
      <ul>
        <li class="grid">
          <div class="griditem">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf" target="_blank" class="imageLink"><img src="static/DemoImages/GRES.jpg"></a><br><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf"  target="_blank" class="imageLink">Paper</a>
        </div>
          </li>
          <!-- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->

        <li class="mygrid">
          <div class="mygriditem">
        <a href="https://github.com/henghuiding/gRefCOCO" target="_blank" class="imageLink"><img src="static/DemoImages/dataset.png"></a><br><a href="https://github.com/henghuiding/gRefCOCO" target="_blank">üî•gRefCOCO Dataset (ready now!)</a>
        </div>
          </li>
          <!-- &nbsp;&nbsp;&nbsp;&nbsp; -->

        <!-- <li li class="mygrid">
          <div class="mygriditem">
        <a href="" target="_blank" class="imageLink"><img src="static/DemoImages/codalab.png"></a><br><a href="https:" target="_blank">Online Evaluation (Coming soon)</a>
        </div>
          </li> -->

        </ul><br><br>

    </div>
</section>




<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      Please consider to cite GRES if it helps your research.
      <pre><code>@inproceedings{GRES,
  title={{GRES}: Generalized Referring Expression Segmentation},
  author={Liu, Chang and Ding, Henghui and Jiang, Xudong},
  booktitle={CVPR},
  year={2023}
}</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->

<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe  src="https://www.youtube.com/embed/COJVqz_sgNU?rel=0&amp;showinfo=0"
            frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <section class="section" id="License">
  <div class="container is-max-desktop content">
  <h2 class="title">License</h2>
  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"  target="_blank"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a></br>
GRES is licensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0 License</a>. The data of gRefCOCO is released for non-commercial research purpose only.
    </div>
</section> -->


<!-- <a href="https://clustrmaps.com/site/1busq" title="Visit tracker" target="_blank"><img src="//www.clustrmaps.com/map_v2.png?d=_VsAxAzBN41nGwpkOyvqVyttIEZSyOpIjSxO4vBjrBA&cl=ffffff" height="1" width="1"/ style="display:block;margin-top:5px;margin-bottom:0px;margin-left:auto;text-align:right"></a> -->
  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>

            <center><font size=2>¬© Henghui Ding | Last updated: 12/3/2024</font></center>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
